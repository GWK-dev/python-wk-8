# cord19_analysis.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from wordcloud import WordCloud
import re
from collections import Counter
from datetime import datetime

class CORD19Analyzer:
    def __init__(self):
        self.df = None
        self.cleaned_df = None
        self.loaded_successfully = False
    
    def load_data(self, filepath=None):
        """
        Load CORD-19 dataset from file or use enhanced sample data
        """
        try:
            if filepath:
                self.df = pd.read_csv(filepath)
                print(f"‚úÖ CORD-19 dataset loaded from {filepath}")
            else:
                self.df = self._create_enhanced_sample_data()
                print("‚úÖ Enhanced sample CORD-19 data loaded")
            
            # Perform comprehensive data cleaning
            self.cleaned_df = self._clean_data_comprehensive(self.df)
            self.loaded_successfully = True
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            return False
    
    def _create_enhanced_sample_data(self):
        """Create enhanced sample data that closely mimics real CORD-19 structure"""
        np.random.seed(42)
        n_samples = 50
        
        dates = pd.date_range('2019-11-01', '2021-08-01', freq='D')
        journals = ['The Lancet', 'Nature', 'Science', 'JAMA', 'BMJ', 'NEJM', 
                   'PLoS One', 'BMC Medicine', 'Lancet Infectious Diseases',
                   'Nature Medicine', 'Science Advances']
        
        research_areas = ['Transmission', 'Clinical', 'Vaccine', 'Treatment', 
                         'Public Health', 'Economic', 'Mental Health', 'Diagnostic']
        
        data = {
            'title': [],
            'abstract': [],
            'publish_time': np.random.choice(dates, n_samples),
            'authors': [],
            'journal': np.random.choice(journals, n_samples),
            'has_pdf_parse': np.random.choice([True, False], n_samples, p=[0.7, 0.3]),
            'has_pmc_xml_parse': np.random.choice([True, False], n_samples, p=[0.6, 0.4]),
            'who_covidence_id': [f"COV_{i:06d}" for i in range(100001, 100001 + n_samples)],
            'source_x': np.random.choice(['PubMed', 'PMC', 'arXiv', 'WHO', 'CZI'], n_samples)
        }
        
        # Generate realistic titles and abstracts
        for i in range(n_samples):
            area = np.random.choice(research_areas)
            if area == 'Transmission':
                title = f"{np.random.choice(['Asymptomatic', 'Community', 'Household'])} transmission of SARS-CoV-2: {np.random.choice(['patterns', 'dynamics', 'risk factors'])}"
                abstract = f"Study investigating {title.lower()} in various population settings with detailed epidemiological analysis."
            elif area == 'Clinical':
                title = f"Clinical {np.random.choice(['characteristics', 'outcomes', 'manifestations'])} of COVID-19 in {np.random.choice(['ICU', 'pediatric', 'geriatric'])} patients"
                abstract = f"Comprehensive analysis of {title.lower()} including laboratory findings and prognostic indicators."
            elif area == 'Vaccine':
                title = f"{np.random.choice(['mRNA', 'viral vector', 'inactivated'])} vaccine {np.random.choice(['efficacy', 'safety', 'immunogenicity'])} against {np.random.choice(['variants', 'severe disease', 'transmission'])}"
                abstract = f"Clinical trial results and real-world evidence for {title.lower()} in diverse populations."
            else:
                title = f"COVID-19 {area.lower()} {np.random.choice(['impact', 'analysis', 'evaluation', 'perspectives'])}"
                abstract = f"Research examining {title.lower()} with methodological rigor and comprehensive data analysis."
            
            data['title'].append(title)
            data['abstract'].append(abstract)
            data['authors'].append(f"Researcher_{i}_A, Researcher_{i}_B, Researcher_{i}_C")
        
        return pd.DataFrame(data)
    
    def _clean_data_comprehensive(self, df):
        """Comprehensive data cleaning and preparation"""
        # Create a copy to avoid modifying original
        cleaned_df = df.copy()
        
        # Convert publish_time to datetime
        cleaned_df['publish_time'] = pd.to_datetime(cleaned_df['publish_time'], errors='coerce')
        
        # Handle missing values strategically
        cleaned_df['abstract'] = cleaned_df['abstract'].fillna('No abstract available')
        cleaned_df['authors'] = cleaned_df['authors'].fillna('Unknown authors')
        cleaned_df['journal'] = cleaned_df['journal'].fillna('Unknown journal')
        
        # Remove rows where critical information is missing
        initial_count = len(cleaned_df)
        cleaned_df = cleaned_df.dropna(subset=['title', 'publish_time'])
        removed_count = initial_count - len(cleaned_df)
        print(f"üìä Removed {removed_count} rows with missing critical data")
        
        # Create analytical features
        cleaned_df['publication_year'] = cleaned_df['publish_time'].dt.year
        cleaned_df['publication_month'] = cleaned_df['publish_time'].dt.month
        cleaned_df['abstract_word_count'] = cleaned_df['abstract'].str.split().str.len()
        cleaned_df['title_word_count'] = cleaned_df['title'].str.split().str.len()
        cleaned_df['has_full_text'] = cleaned_df['has_pdf_parse'] | cleaned_df['has_pmc_xml_parse']
        
        return cleaned_df
    
    def basic_exploration(self):
        """Perform comprehensive data exploration"""
        if not self.loaded_successfully:
            print("No data loaded")
            return
        
        print("=" * 60)
        print("COMPREHENSIVE DATA EXPLORATION")
        print("=" * 60)
        
        # Basic dataset information
        print(f"\nüìê Dataset Dimensions: {self.cleaned_df.shape}")
        print(f"üìÖ Date Range: {self.cleaned_df['publish_time'].min().strftime('%Y-%m-%d')} to {self.cleaned_df['publish_time'].max().strftime('%Y-%m-%d')}")
        
        # Data types and structure
        print("\nüîç Data Types:")
        for col in self.cleaned_df.columns:
            print(f"  - {col}: {self.cleaned_df[col].dtype}")
        
        # Missing values analysis
        print("\nüîé Missing Values Analysis:")
        missing_data = self.cleaned_df.isnull().sum()
        for col, count in missing_data.items():
            if count > 0:
                percentage = (count / len(self.cleaned_df)) * 100
                print(f"  - {col}: {count} ({percentage:.1f}%)")
        
        if missing_data.sum() == 0:
            print("  ‚úÖ No missing values in cleaned dataset")
    
    def perform_analysis(self):
        """Perform comprehensive data analysis"""
        if not self.loaded_successfully:
            print("No data loaded")
            return
        
        print("\n" + "=" * 60)
        print("COMPREHENSIVE DATA ANALYSIS")
        print("=" * 60)
        
        # Publication trends by year
        print("\nüìà Publications by Year:")
        year_counts = self.cleaned_df['publication_year'].value_counts().sort_index()
        for year, count in year_counts.items():
            print(f"  - {year}: {count} papers")
        
        # Top journals
        print("\nüèÜ Top Publishing Journals:")
        top_journals = self.cleaned_df['journal'].value_counts().head(5)
        for journal, count in top_journals.items():
            print(f"  - {journal}: {count} papers")
        
        # Word frequency analysis
        print("\nüî§ Most Frequent Words in Titles:")
        all_titles = ' '.join(self.cleaned_df['title'].dropna().str.lower())
        words = re.findall(r'\b\w+\b', all_titles)
        word_freq = Counter(words)
        
        # Filter out common words
        stop_words = {'the', 'and', 'of', 'in', 'to', 'a', 'for', 'on', 'with', 'as', 'by', 'an', 'at', 'is', 'are'}
        filtered_words = {word: count for word, count in word_freq.items() 
                         if word not in stop_words and len(word) > 3}
        
        top_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:10]
        for word, count in top_words:
            print(f"  - {word}: {count} occurrences")
        
        # Data availability statistics
        print("\nüíæ Data Availability:")
        print(f"  - Papers with PDF: {self.cleaned_df['has_pdf_parse'].sum()} ({self.cleaned_df['has_pdf_parse'].mean()*100:.1f}%)")
        print(f"  - Papers with PMC XML: {self.cleaned_df['has_pmc_xml_parse'].sum()} ({self.cleaned_df['has_pmc_xml_parse'].mean()*100:.1f}%)")
        print(f"  - Papers with any full text: {self.cleaned_df['has_full_text'].sum()} ({self.cleaned_df['has_full_text'].mean()*100:.1f}%)")
    
    def create_comprehensive_visualizations(self):
        """Create all required visualizations"""
        if not self.loaded_successfully:
            print("No data loaded")
            return
        
        print("\nüé® Creating Comprehensive Visualizations...")
        
        # Set up the plotting style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # Create a 2x2 grid with an additional row for word cloud
        fig = plt.figure(figsize=(16, 18))
        gs = fig.add_gridspec(3, 2)
        
        # Plot 1: Publications over time (monthly)
        ax1 = fig.add_subplot(gs[0, 0])
        monthly_data = self.cleaned_df.groupby(self.cleaned_df['publish_time'].dt.to_period('M')).size()
        monthly_data.index = monthly_data.index.astype(str)
        monthly_data.plot(kind='line', ax=ax1, marker='o', linewidth=2, markersize=4, color='#2E86AB')
        ax1.set_title('Monthly Publication Trends', fontweight='bold', fontsize=12)
        ax1.set_xlabel('Month')
        ax1.set_ylabel('Number of Publications')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Top journals
        ax2 = fig.add_subplot(gs[0, 1])
        top_journals = self.cleaned_df['journal'].value_counts().head(8)
        top_journals.plot(kind='barh', ax=ax2, color='#A23B72')
        ax2.set_title('Top Publishing Journals', fontweight='bold', fontsize=12)
        ax2.set_xlabel('Number of Papers')
        
        # Plot 3: Publication year distribution
        ax3 = fig.add_subplot(gs[1, 0])
        year_counts = self.cleaned_df['publication_year'].value_counts().sort_index()
        year_counts.plot(kind='bar', ax=ax3, color='#F18F01', edgecolor='black', alpha=0.8)
        ax3.set_title('Publications by Year', fontweight='bold', fontsize=12)
        ax3.set_xlabel('Year')
        ax3.set_ylabel('Number of Papers')
        
        # Plot 4: Data source distribution
        ax4 = fig.add_subplot(gs[1, 1])
        if 'source_x' in self.cleaned_df.columns:
            source_counts = self.cleaned_df['source_x'].value_counts()
            ax4.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', startangle=90)
            ax4.set_title('Paper Sources Distribution', fontweight='bold', fontsize=12)
        
        # Plot 5: Word cloud
        ax5 = fig.add_subplot(gs[2, :])
        wordcloud = WordCloud(width=800, height=400, background_color='white', 
                             colormap='viridis', max_words=100).generate(' '.join(self.cleaned_df['title']))
        ax5.imshow(wordcloud, interpolation='bilinear')
        ax5.axis('off')
        ax5.set_title('Word Cloud of Research Titles', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.savefig('cord19_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("‚úÖ Comprehensive visualizations saved as 'cord19_comprehensive_analysis.png'")
    
    def generate_insights_report(self):
        """Generate a comprehensive insights report"""
        if not self.loaded_successfully:
            print("No data loaded")
            return
        
        print("\n" + "=" * 60)
        print("COMPREHENSIVE INSIGHTS REPORT")
        print("=" * 60)
        
        insights = []
        
        # Temporal insights
        date_range = self.cleaned_df['publish_time'].max() - self.cleaned_df['publish_time'].min()
        insights.append(f"üìÖ Research spans {date_range.days} days, indicating rapid research response")
        
        # Content insights
        avg_abstract_words = self.cleaned_df['abstract_word_count'].mean()
        insights.append(f"üìù Average abstract length: {avg_abstract_words:.1f} words, suggesting detailed study descriptions")
        
        # Collaboration patterns
        avg_authors = self.cleaned_df['authors'].str.split(',').apply(len).mean()
        insights.append(f"üë• Collaborative research: {avg_authors:.1f} authors per paper on average")
        
        # Research focus areas
        clinical_keywords = ['clinical', 'patient', 'treatment', 'therapy']
        clinical_count = self.cleaned_df['title'].str.lower().str.contains('|'.join(clinical_keywords)).sum()
        insights.append(f"üî¨ Clinical focus: {clinical_count} papers ({clinical_count/len(self.cleaned_df)*100:.1f}%) focus on clinical aspects")
        
        # Data accessibility
        full_text_available = self.cleaned_df['has_full_text'].sum()
        insights.append(f"üíæ Data accessibility: {full_text_available} papers ({full_text_available/len(self.cleaned_df)*100:.1f}%) have full text available")
        
        # Print insights
        for i, insight in enumerate(insights, 1):
            print(f"{i}. {insight}")

def main():
    """Main function to run the comprehensive analysis"""
    analyzer = CORD19Analyzer()
    
    print("üî¨ CORD-19 COMPREHENSIVE RESEARCH ANALYSIS")
    print("=" * 50)
    
    # Load data
    print("Loading and cleaning data...")
    success = analyzer.load_data()
    
    if success:
        # Perform all analysis steps
        analyzer.basic_exploration()
        analyzer.perform_analysis()
        analyzer.create_comprehensive_visualizations()
        analyzer.generate_insights_report()
        
        print("\n" + "=" * 50)
        print("‚úÖ COMPREHENSIVE ANALYSIS COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        print("\nüìã Summary of Completed Tasks:")
        print("‚úì Part 1: Data loading and basic exploration")
        print("‚úì Part 2: Comprehensive data cleaning and preparation") 
        print("‚úì Part 3: Advanced data analysis and insights generation")
        print("‚úì Part 4: Multiple comprehensive visualizations created")
        print("‚úì Part 5: Detailed insights report generated")
        print("\nüìä Generated Files:")
        print("  - cord19_comprehensive_analysis.png (Visualizations)")
        print("  - Enhanced dataset with analytical features")

if __name__ == "__main__":
    main()